{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "494a8c17"
      },
      "source": [
        "# CS 584 Assignment 3 -- Language Model\n",
        "\n",
        "#### Name: Aishwarya Bhethanabotla\n",
        "#### Stevens ID: 20027553"
      ],
      "id": "494a8c17"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DExq2eLGSoC6",
        "outputId": "da08aeb5-2841-4f47-93e1-dd2c518f4a22"
      },
      "id": "DExq2eLGSoC6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a2591a0"
      },
      "source": [
        "## In this assignment, you are required to follow the steps below:\n",
        "1. Review the lecture slides.\n",
        "2. Implement N-gram language modeling.\n",
        "3. Implement RNN language modeling.\n",
        "\n",
        "**Before you start**\n",
        "- Please read the code very carefully.\n",
        "- Install these packages (jupyterlab, matplotlib, nltk, numpy, scikit-learn, tensorflow, tensorflow_addons) using the following command.\n",
        "```console\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "- It's better to train the Tensorflow model with GPU and CUDA. If they are not available on your local machine, please consider Google CoLab. You can check `CoLab.md` in this assignments.\n",
        "- You are **NOT** allowed to use other packages unless otherwise specified.\n",
        "- You are **ONLY** allowed to edit the code between `# Start your code here` and `# End` for each block."
      ],
      "id": "9a2591a0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAm1w3aHScY-"
      },
      "source": [
        "## Part A: 1. N-Gram (60 Points)"
      ],
      "id": "wAm1w3aHScY-"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqcPDuLVVKNp",
        "outputId": "ded0a653-65c6-4902-8ba0-8c7c1c69c8aa"
      },
      "id": "bqcPDuLVVKNp",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jupyterlab (from -r requirements.txt (line 1))\n",
            "  Downloading jupyterlab-4.4.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (1.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (2.18.0)\n",
            "Collecting tensorflow_addons (from -r requirements.txt (line 6))\n",
            "  Downloading tensorflow_addons-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab->-r requirements.txt (line 1))\n",
            "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->-r requirements.txt (line 1)) (0.28.1)\n",
            "Requirement already satisfied: ipykernel>=6.5.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->-r requirements.txt (line 1)) (6.17.1)\n",
            "Requirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.11/dist-packages (from jupyterlab->-r requirements.txt (line 1)) (5.7.2)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->-r requirements.txt (line 1))\n",
            "  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->-r requirements.txt (line 1))\n",
            "  Downloading jupyter_server-2.15.0-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->-r requirements.txt (line 1))\n",
            "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->-r requirements.txt (line 1)) (0.2.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from jupyterlab->-r requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: setuptools>=41.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->-r requirements.txt (line 1)) (75.2.0)\n",
            "Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->-r requirements.txt (line 1)) (6.4.2)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.11/dist-packages (from jupyterlab->-r requirements.txt (line 1)) (5.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 2)) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 2)) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 2)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 4)) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 4)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 4)) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 5)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 5)) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 5)) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 5)) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 5)) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 5)) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 5)) (2.32.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 5)) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 5)) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 5)) (4.13.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 5)) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 5)) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 5)) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 5)) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 5)) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 5)) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 5)) (0.37.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow_addons->-r requirements.txt (line 6))\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 5)) (0.45.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->-r requirements.txt (line 1)) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->-r requirements.txt (line 1)) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->-r requirements.txt (line 1)) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->-r requirements.txt (line 1)) (0.14.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 1)) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 1)) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 1)) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 1)) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 1)) (24.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=3.0.3->jupyterlab->-r requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: argon2-cffi>=21.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (23.1.0)\n",
            "Collecting jupyter-client>=6.1.12 (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 1))\n",
            "  Downloading jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1))\n",
            "  Downloading jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1))\n",
            "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (7.16.6)\n",
            "Requirement already satisfied: nbformat>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (5.10.4)\n",
            "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1))\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: prometheus-client>=0.9 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (0.21.1)\n",
            "Requirement already satisfied: send2trash>=1.8.2 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (0.18.1)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core->jupyterlab->-r requirements.txt (line 1)) (4.3.7)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->-r requirements.txt (line 1)) (2.17.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->-r requirements.txt (line 1))\n",
            "  Downloading json5-0.12.0-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->-r requirements.txt (line 1)) (4.23.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->-r requirements.txt (line 5)) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->-r requirements.txt (line 5)) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->-r requirements.txt (line 5)) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->-r requirements.txt (line 5)) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->-r requirements.txt (line 5)) (2.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->-r requirements.txt (line 5)) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->-r requirements.txt (line 5)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->-r requirements.txt (line 5)) (3.1.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.0->jupyterlab->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (21.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 1))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 1)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 1)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 1)) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 1)) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 1)) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->-r requirements.txt (line 1)) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->-r requirements.txt (line 1)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->-r requirements.txt (line 1)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->-r requirements.txt (line 1)) (0.24.0)\n",
            "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1))\n",
            "  Downloading python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (6.0.2)\n",
            "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1))\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1))\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (4.13.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (2.21.1)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.11/dist-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->-r requirements.txt (line 5)) (3.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 1)) (0.8.4)\n",
            "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1))\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1))\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (3.0.0)\n",
            "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1))\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (24.11.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow->-r requirements.txt (line 5)) (0.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 1)) (0.2.13)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (2.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1)) (2.22)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1))\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 1))\n",
            "  Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
            "Downloading jupyterlab-4.4.0-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_addons-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_server-2.15.0-py3-none-any.whl (385 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.8/385.8 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Downloading json5-0.12.0-py3-none-any.whl (36 kB)\n",
            "Downloading jupyter_client-8.6.3-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
            "Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
            "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: uri-template, types-python-dateutil, typeguard, rfc3986-validator, rfc3339-validator, python-json-logger, overrides, json5, jedi, fqdn, async-lru, tensorflow_addons, jupyter-server-terminals, jupyter-client, arrow, isoduration, jupyter-events, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.4.2\n",
            "    Uninstalling typeguard-4.4.2:\n",
            "      Successfully uninstalled typeguard-4.4.2\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 6.1.12\n",
            "    Uninstalling jupyter-client-6.1.12:\n",
            "      Successfully uninstalled jupyter-client-6.1.12\n",
            "  Attempting uninstall: jupyter-server\n",
            "    Found existing installation: jupyter-server 1.16.0\n",
            "    Uninstalling jupyter-server-1.16.0:\n",
            "      Successfully uninstalled jupyter-server-1.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 7.5.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\n",
            "notebook 6.5.7 requires jupyter-client<8,>=5.3.4, but you have jupyter-client 8.6.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed arrow-1.3.0 async-lru-2.0.5 fqdn-1.5.1 isoduration-20.11.0 jedi-0.19.2 json5-0.12.0 jupyter-client-8.6.3 jupyter-events-0.12.0 jupyter-lsp-2.2.5 jupyter-server-2.15.0 jupyter-server-terminals-0.5.3 jupyterlab-4.4.0 jupyterlab-server-2.27.3 overrides-7.7.0 python-json-logger-3.3.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 tensorflow_addons-0.23.0 typeguard-2.13.3 types-python-dateutil-2.9.0.20241206 uri-template-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xPnEto4WScY-"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "\n",
        "def print_line(*args):\n",
        "    \"\"\" Inline print and go to the begining of line\n",
        "    \"\"\"\n",
        "    args1 = [str(arg) for arg in args]\n",
        "    str_ = ' '.join(args1)\n",
        "    print('\\r' + str_, end='')"
      ],
      "id": "xPnEto4WScY-"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-i-HxmgScY-",
        "outputId": "f7a8fc5b-0f33-484f-cdb4-1089894960d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# If you are going to use GPU, make sure the GPU is in the output\n",
        "tf.config.list_physical_devices('GPU')"
      ],
      "id": "I-i-HxmgScY-"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FTKEjiFJScY-"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Union, Dict\n",
        "\n",
        "import numpy as np"
      ],
      "id": "FTKEjiFJScY-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "681c2be5"
      },
      "source": [
        "### 1.1 Load Data & Preprocessing"
      ],
      "id": "681c2be5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iZkJEUwScY_"
      },
      "source": [
        "You will not need to implement the data preprocessing."
      ],
      "id": "_iZkJEUwScY_"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6dca548c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18c7e2d5-7f3d-445b-92d8-4b38a38eaf32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of train sentences: 42068\n",
            "number of valid sentences: 3370\n",
            "number of test sentences: 3165\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "\n",
        "data_path = '/content/a3-data'\n",
        "\n",
        "train_sentences = open(os.path.join(data_path, 'train.txt')).readlines()\n",
        "valid_sentences = open(os.path.join(data_path, 'valid.txt')).readlines()\n",
        "test_sentences = open(os.path.join(data_path, 'input.txt')).readlines()\n",
        "print('number of train sentences:', len(train_sentences))\n",
        "print('number of valid sentences:', len(valid_sentences))\n",
        "print('number of test sentences:', len(test_sentences))"
      ],
      "id": "6dca548c"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fcUDIDVuScY_"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "class Preprocessor:\n",
        "    def __init__(self, punctuation=True, url=True, number=True):\n",
        "        self.punctuation = punctuation\n",
        "        self.url = url\n",
        "        self.number = number\n",
        "\n",
        "    def apply(self, sentence: str) -> str:\n",
        "        \"\"\" Apply the preprocessing rules to the sentence\n",
        "        Args:\n",
        "            sentence: raw sentence\n",
        "        Returns:\n",
        "            sentence: clean sentence\n",
        "        \"\"\"\n",
        "        sentence = sentence.lower()\n",
        "        sentence = sentence.replace('<unk>', '')\n",
        "        if self.url:\n",
        "            sentence = Preprocessor.remove_url(sentence)\n",
        "        if self.punctuation:\n",
        "            sentence = Preprocessor.remove_punctuation(sentence)\n",
        "        if self.number:\n",
        "            sentence = Preprocessor.remove_number(sentence)\n",
        "        sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "        return sentence\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_punctuation(sentence: str) -> str:\n",
        "        \"\"\" Remove punctuations in sentence with re\n",
        "        Args:\n",
        "            sentence: sentence with possible punctuations\n",
        "        Returns:\n",
        "            sentence: sentence without punctuations\n",
        "        \"\"\"\n",
        "        sentence = re.sub(r'[^\\w\\s]', ' ', sentence)\n",
        "        return sentence\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_url(sentence: str) -> str:\n",
        "        \"\"\" Remove urls in text with re\n",
        "        Args:\n",
        "            sentence: sentence with possible urls\n",
        "        Returns:\n",
        "            sentence: sentence without urls\n",
        "        \"\"\"\n",
        "        sentence = re.sub(r'(https|http)?://(\\w|\\.|/|\\?|=|&|%)*\\b', ' ', sentence)\n",
        "        return sentence\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_number(sentence: str) -> str:\n",
        "        \"\"\" Remove numbers in sentence with re\n",
        "        Args:\n",
        "            sentence: sentence with possible numbers\n",
        "        Returns:\n",
        "            sentence: sentence without numbers\n",
        "        \"\"\"\n",
        "        sentence = re.sub(r'\\d+', ' ', sentence)\n",
        "        return sentence"
      ],
      "id": "fcUDIDVuScY_"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "O53cOf7VScY_"
      },
      "outputs": [],
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self, sos_token='<s>', eos_token='</s>', pad_token='<pad>', unk_token='<unk>', mask_token='<mask>'):\n",
        "        # Special tokens.\n",
        "        self.sos_token = sos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.pad_token = pad_token\n",
        "        self.unk_token = unk_token\n",
        "        self.mask_token = mask_token\n",
        "\n",
        "        self.vocab = { sos_token: 0, eos_token: 1, pad_token: 2, unk_token: 3, mask_token: 4 }\n",
        "        self.inverse_vocab = { 0: sos_token, 1: eos_token, 2: pad_token, 3: unk_token, 4: mask_token }\n",
        "        self.token_occurrence = { sos_token: 0, eos_token: 0, pad_token: 0, unk_token: 0, mask_token: 0 }\n",
        "\n",
        "        self.preprocessor = Preprocessor()\n",
        "\n",
        "    @property\n",
        "    def sos_token_id(self):\n",
        "        \"\"\" Create a property method.\n",
        "            You can use self.sos_token_id or tokenizer.sos_token_id to get the id of the sos_token.\n",
        "        \"\"\"\n",
        "        return self.vocab[self.sos_token]\n",
        "\n",
        "    @property\n",
        "    def eos_token_id(self):\n",
        "        return self.vocab[self.eos_token]\n",
        "\n",
        "    @property\n",
        "    def pad_token_id(self):\n",
        "        return self.vocab[self.pad_token]\n",
        "\n",
        "    @property\n",
        "    def unk_token_id(self):\n",
        "        return self.vocab[self.unk_token]\n",
        "\n",
        "    @property\n",
        "    def mask_token_id(self):\n",
        "        return self.vocab[self.mask_token]\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\" A magic method that enable program to know the number of tokens by calling:\n",
        "            ```python\n",
        "            tokenizer = Tokenizer()\n",
        "            num_tokens = len(tokenizer)\n",
        "            ```\n",
        "        \"\"\"\n",
        "        return len(self.vocab)\n",
        "\n",
        "    def fit(self, sentences: List[str]):\n",
        "        \"\"\" Fit the tokenizer using all sentences.\n",
        "        1. Tokenize the sentence by splitting with spaces.\n",
        "        2. Record the occurrence of all tokens\n",
        "        3. Construct the token to index (self.vocab) map and the inversed map (self.inverse_vocab) based on the occurrence. The token with a higher occurrence has the smaller index\n",
        "\n",
        "        Args:\n",
        "            sentences: All sentences in the dataset.\n",
        "        \"\"\"\n",
        "        n = len(sentences)\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            if i % 100 == 0 or i == n - 1:\n",
        "                print_line('Fitting Tokenizer:', (i + 1), '/', n)\n",
        "            tokens = self.preprocessor.apply(sentence.strip()).split()\n",
        "            if len(tokens) <= 1:\n",
        "                continue\n",
        "            for token in tokens:\n",
        "                if token == '<unk>':\n",
        "                    continue\n",
        "                self.token_occurrence[token] = self.token_occurrence.get(token, 0) + 1\n",
        "        print_line('\\n')\n",
        "\n",
        "        token_occurrence = sorted(self.token_occurrence.items(), key=lambda e: e[1], reverse=True)\n",
        "        for token, occurrence in token_occurrence[:-5]:\n",
        "            token_id = len(self.vocab)\n",
        "            self.vocab[token] = token_id\n",
        "            self.inverse_vocab[token_id] = token\n",
        "\n",
        "        print('The number of distinct tokens:', len(self.vocab))\n",
        "\n",
        "    def encode(self, sentences: List[str]) -> List[List[int]]:\n",
        "        \"\"\" Encode the sentences into token ids\n",
        "            Note: 1. if a token in a sentence does not exist in the fit encoder, we ignore it.\n",
        "                  2. If the number of tokens in a sentence is less than two, we ignore this sentence.\n",
        "                  3. Note that, for every sentence, we will add an sos_token, i.e., the id of <s> at the start of the sentence,\n",
        "                     and add an eos_token, i.e., the id of </s> at the end of the sentence.\n",
        "        Args:\n",
        "            sentences: Raw sentences\n",
        "        Returns:\n",
        "            sent_token_ids: A list of id list\n",
        "        \"\"\"\n",
        "        n = len(sentences)\n",
        "        sent_token_ids = []\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            if i % 100 == 0 or i == n - 1:\n",
        "                print_line('Encoding with Tokenizer:', (i + 1), '/', n)\n",
        "            token_ids = []\n",
        "            tokens = self.preprocessor.apply(sentence.strip()).split()\n",
        "            for token in tokens:\n",
        "                if token == '<unk>':\n",
        "                    continue\n",
        "                if token in self.vocab:\n",
        "                    token_ids.append(self.vocab[token])\n",
        "            if len(token_ids) <= 1:\n",
        "                continue\n",
        "            token_ids = [self.sos_token_id] + token_ids + [self.eos_token_id]\n",
        "            sent_token_ids.append(token_ids)\n",
        "        print_line('\\n')\n",
        "        return sent_token_ids"
      ],
      "id": "O53cOf7VScY_"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyLVAaUSScY_",
        "outputId": "6b6d7b45-2aa9-4d26-cdb6-85d5858d592f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rFitting Tokenizer: 1 / 2\rFitting Tokenizer: 2 / 2\r\n",
            "The number of distinct tokens: 44\n",
            "\n",
            "n : 2\n",
            "aer : 1\n",
            "banknote : 1\n",
            "berlitz : 1\n",
            "calloway : 1\n",
            "centrust : 1\n",
            "cluett : 1\n",
            "fromstein : 1\n",
            "gitano : 1\n",
            "guterman : 1\n",
            "\n",
            "\rEncoding with Tokenizer: 1 / 2\rEncoding with Tokenizer: 2 / 2\r\n",
            "\n",
            " aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \n",
            " ['<s>', 'aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro', 'quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack', 'food', 'ssangyong', 'swapo', 'wachter', '</s>'] \n",
            "\n",
            " pierre <unk> N years old will join the board as a nonexecutive director nov. N \n",
            " ['<s>', 'pierre', 'n', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov', 'n', '</s>'] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit(train_sentences[:2])\n",
        "print()\n",
        "\n",
        "token_occurrence = sorted(tokenizer.token_occurrence.items(), key=lambda e: e[1], reverse=True)\n",
        "for token, occurrence in token_occurrence[:10]:\n",
        "    print(token, ':', occurrence)\n",
        "print()\n",
        "sent_token_ids = tokenizer.encode(train_sentences[:2])\n",
        "print()\n",
        "for original_sentence, token_ids in zip(train_sentences[:2], sent_token_ids):\n",
        "    sentence = [tokenizer.inverse_vocab[token] for token in token_ids]\n",
        "    print(original_sentence, sentence, '\\n')"
      ],
      "id": "IyLVAaUSScY_"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNTN1-caScY_",
        "outputId": "7c3bed6b-0435-4acd-de8a-b1909b4dfda7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting Tokenizer: 42068 / 42068\n",
            "The number of distinct tokens: 9614\n",
            "Encoding with Tokenizer: 42068 / 42068\n",
            "Encoding with Tokenizer: 3370 / 3370\n",
            "Encoding with Tokenizer: 3165 / 3165\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit(train_sentences)\n",
        "train_token_ids = tokenizer.encode(train_sentences)\n",
        "valid_token_ids = tokenizer.encode(valid_sentences)\n",
        "test_token_ids = tokenizer.encode(test_sentences)"
      ],
      "id": "vNTN1-caScY_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3121ada5"
      },
      "source": [
        "### 1.2 Calculate unigram and bigram count (10 points)"
      ],
      "id": "3121ada5"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "17nw-bDTScY_"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict\n",
        "\n",
        "def get_unigram_count(train_token_ids: List[List[int]]) -> Dict:\n",
        "    \"\"\" Calculate the occurrence of each token in the dataset.\n",
        "\n",
        "    Args:\n",
        "        train_token_ids: each element is a list of token ids\n",
        "    Return:\n",
        "        unigram_count: A map from token_id to occurrence\n",
        "    \"\"\"\n",
        "    unigram_count = {}\n",
        "    for sent in train_token_ids:\n",
        "        for token_id in sent:\n",
        "            unigram_count[token_id] = unigram_count.get(token_id, 0) + 1\n",
        "    return unigram_count\n"
      ],
      "id": "17nw-bDTScY_"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hADW5cKlScZA"
      },
      "outputs": [],
      "source": [
        "def get_bigram_count(train_token_ids: List[List[int]]) -> Dict[int, Dict]:\n",
        "    \"\"\" Calculate the occurrence of bigrams in the dataset.\n",
        "\n",
        "    Args:\n",
        "        train_token_ids: each element is a list of token ids\n",
        "    Return:\n",
        "        bigram_count: A map from token_id to next token occurrence. Key: token_id, value: Dict[token_id -> occurrence]\n",
        "                      For example, {\n",
        "                          5: {10: 5, 20: 4}\n",
        "                      } means (5, 10) occurs 5 times and (5, 20) occurs 4 times.\n",
        "    \"\"\"\n",
        "    bigram_count = {}\n",
        "    for sent in train_token_ids:\n",
        "\n",
        "        for i in range(len(sent) - 1):\n",
        "            current_token = sent[i]\n",
        "            next_token = sent[i + 1]\n",
        "            if current_token not in bigram_count:\n",
        "                bigram_count[current_token] = {}\n",
        "            bigram_count[current_token][next_token] = bigram_count[current_token].get(next_token, 0) + 1\n",
        "    return bigram_count\n"
      ],
      "id": "hADW5cKlScZA"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uoXM720DScZA"
      },
      "outputs": [],
      "source": [
        "unigram_count = get_unigram_count(train_token_ids)\n",
        "bigram_count = get_bigram_count(train_token_ids)"
      ],
      "id": "uoXM720DScZA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVhCjgnCScZA"
      },
      "source": [
        "### 1.3 BiGram (5 points)"
      ],
      "id": "zVhCjgnCScZA"
    },
    {
      "cell_type": "code",
      "source": [
        "class BiGram:\n",
        "    def __init__(self, unigram_count, bigram_count):\n",
        "        self.unigram_count = unigram_count\n",
        "        self.bigram_count = bigram_count\n",
        "\n",
        "    def calc_prob(self, w1: int, w2: int) -> float:\n",
        "        \"\"\" Calculate the probability of p(w2 | w1) using the BiGram model.\n",
        "\n",
        "        Args:\n",
        "            w1, w2: current token and next token\n",
        "        Note:\n",
        "            if the calculated probability is 0, return 1e-5.\n",
        "        \"\"\"\n",
        "        # If w1 is not found or w2 never follows w1, return a tiny probability.\n",
        "        if w1 not in self.bigram_count or w2 not in self.bigram_count[w1]:\n",
        "            return 1e-5\n",
        "\n",
        "        # conditional probability\n",
        "        prob = self.bigram_count[w1][w2] / self.unigram_count[w1]\n",
        "\n",
        "\n",
        "        if prob == 0:\n",
        "            return 1e-5\n",
        "\n",
        "        return prob"
      ],
      "metadata": {
        "id": "AE70knMLbcmP"
      },
      "id": "AE70knMLbcmP",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bENwtJU0ScZA"
      },
      "source": [
        "### 1.4 Good Turing (15 points)"
      ],
      "id": "bENwtJU0ScZA"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UPEDFfHWScZA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from scipy.optimize import curve_fit\n",
        "from typing import Dict, Tuple, Union\n",
        "\n",
        "def power_law(x, a, b):\n",
        "    \"\"\"Power law function to fit the number of occurrences.\"\"\"\n",
        "    return a * np.power(x, b)\n",
        "\n",
        "\n",
        "\n",
        "class GoodTuring(BiGram):\n",
        "    def __init__(self, unigram_count, bigram_count, threshold=100, T_cap=200):\n",
        "        \"\"\"\n",
        "        T_cap: Maximum effective number of unseen continuation types to consider\n",
        "               when distributing reserved probability mass.\n",
        "        \"\"\"\n",
        "        super().__init__(unigram_count, bigram_count)\n",
        "        self.threshold = threshold\n",
        "        self.T_cap = T_cap\n",
        "        self.bigram_Nc = self.calc_Nc()\n",
        "        self.bi_c_star, self.bi_N = self.smoothing(self.bigram_Nc)\n",
        "        # V_cont: no.  of distinct tokens that have ever appeared as a continuation\n",
        "        self.V_cont = len({w2 for w1 in self.bigram_count for w2 in self.bigram_count[w1]})\n",
        "\n",
        "    def calc_Nc(self) -> Dict[int, Union[float, int]]:\n",
        "        \"\"\"Calculate Nc of bigrams.\n",
        "\n",
        "        Returns:\n",
        "            bigram_Nc: A map from count to its frequency.\n",
        "                       For example, {10: 78} means 78 bigrams occur 10 times.\n",
        "                       For counts above the threshold, frequency is replaced using a power law.\n",
        "        \"\"\"\n",
        "        bigram_Nc = {}\n",
        "        for w1 in self.bigram_count:\n",
        "            for w2, count in self.bigram_count[w1].items():\n",
        "                bigram_Nc[count] = bigram_Nc.get(count, 0) + 1\n",
        "\n",
        "        self.replace_large_c(bigram_Nc)\n",
        "        return bigram_Nc\n",
        "\n",
        "    def replace_large_c(self, Nc):\n",
        "        \"\"\"Replace counts for large c with a power law fit.\"\"\"\n",
        "        x, y = zip(*sorted(Nc.items(), reverse=True))\n",
        "        popt, _ = curve_fit(power_law, x, y, bounds=([0, -np.inf], [np.inf, 0]))\n",
        "        a, b = popt\n",
        "        max_count = max(Nc.keys())\n",
        "        for c in range(self.threshold + 1, max_count + 2):\n",
        "            Nc[c] = power_law(c, a, b)\n",
        "\n",
        "    def smoothing(self, Nc: Dict[int, Union[float, int]]) -> Tuple[Dict[int, float], float]:\n",
        "        \"\"\"Calculate Good-Turing smoothed counts (c_star) and total mass N.\n",
        "\n",
        "        Args:\n",
        "            Nc: Map from bigram count to its frequency (after replacement for large counts)\n",
        "        Returns:\n",
        "            c_star: Mapping from original bigram count to smoothed count.\n",
        "                    For counts below the threshold, c* = (c+1)*(Nc[c+1]/Nc[c]).\n",
        "                    For counts >= threshold, leave the count unchanged.\n",
        "            N: Total mass computed as sum(c * Nc[c]) over all counts.\n",
        "        \"\"\"\n",
        "        c_star = {}\n",
        "        N = 0\n",
        "        for c in sorted(Nc.keys()):\n",
        "            N += c * Nc[c]\n",
        "            if c < self.threshold:\n",
        "                c_star[c] = (c + 1) * (Nc.get(c + 1, 0) / Nc[c])\n",
        "            else:\n",
        "                c_star[c] = c\n",
        "        return c_star, N\n",
        "\n",
        "    def calc_prob(self, w1, w2) -> float:\n",
        "        \"\"\"Calculate the probability p(w2 | w1) using the Good-Turing model.\n",
        "\n",
        "        For observed bigrams, use the smoothed count.\n",
        "        For unseen bigrams, reserve probability mass from count-1 events and distribute it\n",
        "        uniformly over unseen continuation types (capped by T_cap).\n",
        "\n",
        "        Args:\n",
        "            w1, w2: current token and next token (token ids)\n",
        "        \"\"\"\n",
        "        if w1 in self.unigram_count:\n",
        "            C_w1 = self.unigram_count[w1]\n",
        "        else:\n",
        "            return 1e-5\n",
        "\n",
        "        if w1 in self.bigram_count:\n",
        "            if w2 in self.bigram_count[w1]:\n",
        "                c = self.bigram_count[w1][w2]\n",
        "                c_star = self.bi_c_star.get(c, c)\n",
        "                prob = c_star / C_w1\n",
        "            else:\n",
        "                # Unseen bigram: compute reserved mass from count-1 events.\n",
        "                N1 = sum(1 for count in self.bigram_count[w1].values() if count == 1)\n",
        "                p_unseen_total = N1 / C_w1\n",
        "                seen_continuations = len(self.bigram_count[w1])\n",
        "                T = self.V_cont - seen_continuations\n",
        "                # Cap T to T_cap.\n",
        "                T_eff = T if T < self.T_cap else self.T_cap\n",
        "                if T_eff > 0:\n",
        "                    prob = p_unseen_total / T_eff\n",
        "                else:\n",
        "                    prob = 1e-5\n",
        "        else:\n",
        "            prob = 1e-5\n",
        "\n",
        "        return max(prob, 1e-5)\n",
        "\n",
        "def perplexity(model, token_ids):\n",
        "    \"\"\"Calculate the perplexity score.\n",
        "\n",
        "    Args:\n",
        "        model: the language model (e.g., BiGram, GoodTuring, or KneserNey)\n",
        "        token_ids: list of token id lists (one per sentence)\n",
        "    Returns:\n",
        "        perplexity: the perplexity value\n",
        "    \"\"\"\n",
        "    log_probs = 0\n",
        "    n_words = 0\n",
        "    n_sentences = len(token_ids)\n",
        "    for i, tokens in enumerate(token_ids):\n",
        "        if i % 100 == 0 or i == n_sentences - 1:\n",
        "            print(f'Calculating perplexity: {i+1} / {n_sentences}')\n",
        "        for j in range(1, len(tokens)):\n",
        "            prob = model.calc_prob(tokens[j-1], tokens[j])\n",
        "            log_probs += math.log(prob)\n",
        "            n_words += 1\n",
        "    perp = math.exp(-log_probs / n_words) if n_words > 0 else float('inf')\n",
        "    print()\n",
        "    return perp\n"
      ],
      "id": "UPEDFfHWScZA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKUjEEX5ScZA"
      },
      "source": [
        "### 1.5 Kneser-Ney (15 points)"
      ],
      "id": "TKUjEEX5ScZA"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xZxsgCXlScZA"
      },
      "outputs": [],
      "source": [
        "class KneserNey(BiGram):\n",
        "    def __init__(self, unigram_count, bigram_count, d=0.75):\n",
        "        super().__init__(unigram_count, bigram_count)\n",
        "        self.d = d\n",
        "        self.lambda_ = self.calc_lambda()\n",
        "        self.p_continuation = self.calc_p_continuation()\n",
        "\n",
        "    def calc_lambda(self):\n",
        "        \"\"\"Calculate λ(w1) for each token w1.\n",
        "\n",
        "        λ(w1) = (d / count(w1)) * (number of unique bigrams starting with w1)\n",
        "\n",
        "        Returns:\n",
        "            lambda_: A dict mapping token_id (w1) to its λ(w1) value.\n",
        "        \"\"\"\n",
        "        lambda_ = {}\n",
        "        for w1 in self.bigram_count:\n",
        "            unique_continuations = len(self.bigram_count[w1])\n",
        "            if self.unigram_count[w1] > 0:\n",
        "                lambda_[w1] = (self.d * unique_continuations) / self.unigram_count[w1]\n",
        "            else:\n",
        "                lambda_[w1] = 0\n",
        "        return lambda_\n",
        "\n",
        "    def calc_p_continuation(self):\n",
        "        \"\"\"Calculate the continuation probability p_continuation(w).\n",
        "\n",
        "        p_continuation(w) = (number of unique preceding tokens for w) / V\n",
        "\n",
        "        Here V is the vocabulary size (i.e. the number of tokens that ever appear as a word).\n",
        "        This normalization (instead of using the total number of unique bigrams)\n",
        "        boosts the back‑off probability.\n",
        "\n",
        "        Returns:\n",
        "            p_continuation: A dict mapping token_id (w) to p_continuation(w).\n",
        "        \"\"\"\n",
        "        contexts = {}\n",
        "        for w1 in self.bigram_count:\n",
        "            for w2 in self.bigram_count[w1]:\n",
        "                if w2 not in contexts:\n",
        "                    contexts[w2] = set()\n",
        "                contexts[w2].add(w1)\n",
        "\n",
        "        # For each token w, count the number of unique histories (preceding tokens).\n",
        "        numerator = { w: len(contexts[w]) for w in contexts }\n",
        "\n",
        "        V = len(self.unigram_count)\n",
        "        p_continuation = { w: (numerator[w] / V) for w in numerator }\n",
        "        return p_continuation\n",
        "\n",
        "    def calc_prob(self, w1, w2) -> float:\n",
        "        \"\"\"Calculate the probability p(w2 | w1) using the Kneser‑Ney model.\n",
        "\n",
        "        p_KN(w2|w1) = max{ c(w1, w2) - d, 0 } / count(w1)\n",
        "                        + λ(w1) * p_continuation(w2)\n",
        "\n",
        "        Args:\n",
        "            w1, w2: current token and next token (token ids)\n",
        "        Returns:\n",
        "            p: The Kneser‑Ney probability of w2 given w1.\n",
        "        \"\"\"\n",
        "        c_w1_w2 = self.bigram_count[w1][w2] if (w1 in self.bigram_count and w2 in self.bigram_count[w1]) else 0\n",
        "        base_prob = self.p_continuation.get(w2, 0)\n",
        "        p = max(c_w1_w2 - self.d, 0) / self.unigram_count[w1] + self.lambda_.get(w1, 0) * base_prob\n",
        "        return p if p > 0 else 1e-5\n"
      ],
      "id": "xZxsgCXlScZA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7YvXguaScZA"
      },
      "source": [
        "### Show that perplexity is the exponential of the total loss divided by the number of predictions."
      ],
      "id": "V7YvXguaScZA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-UzIMNrScZA"
      },
      "source": [
        "### 1.6 Perplexity (10 points)"
      ],
      "id": "2-UzIMNrScZA"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "IPjI480eScZB"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def perplexity(model, token_ids):\n",
        "    \"\"\" Calculate the perplexity score.\n",
        "\n",
        "    Args:\n",
        "        model: the model you want to evaluate (BiGram, GoodTuring, or KneserNey)\n",
        "        token_ids: a list of validation token_ids, where each element is a list of token ids for a sentence.\n",
        "    Return:\n",
        "        perplexity: the perplexity of the model on the texts\n",
        "    \"\"\"\n",
        "    log_probs = 0\n",
        "    n = len(token_ids)\n",
        "    n_words = 0\n",
        "    for i, tokens in enumerate(token_ids):\n",
        "        if i % 100 == 0 or i == n - 1:\n",
        "            print_line('Calculating perplexity:', (i + 1), '/', n)\n",
        "        # Iterate over each bigram in the sentence.\n",
        "        for j in range(1, len(tokens)):\n",
        "            # Calculate the probability of the current token given the previous one.\n",
        "            prob = model.calc_prob(tokens[j-1], tokens[j])\n",
        "\n",
        "            log_probs += math.log(prob)\n",
        "            n_words += 1\n",
        "\n",
        "    #perplexity using the formula: exp(-average log probability)\n",
        "    perp = math.exp(-log_probs / n_words) if n_words > 0 else float('inf')\n",
        "    print('\\n')\n",
        "    return perp\n"
      ],
      "id": "IPjI480eScZB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrzYANgCScZB"
      },
      "source": [
        "If you implement correctly, the perplexity of bigram will be around 320"
      ],
      "id": "YrzYANgCScZB"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2GFrhGgScZB",
        "outputId": "64aefd62-3643-4c86-fad7-9abefb6d0096"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rCalculating perplexity: 1 / 3352\rCalculating perplexity: 101 / 3352\rCalculating perplexity: 201 / 3352\rCalculating perplexity: 301 / 3352\rCalculating perplexity: 401 / 3352\rCalculating perplexity: 501 / 3352\rCalculating perplexity: 601 / 3352\rCalculating perplexity: 701 / 3352\rCalculating perplexity: 801 / 3352\rCalculating perplexity: 901 / 3352\rCalculating perplexity: 1001 / 3352\rCalculating perplexity: 1101 / 3352\rCalculating perplexity: 1201 / 3352\rCalculating perplexity: 1301 / 3352\rCalculating perplexity: 1401 / 3352\rCalculating perplexity: 1501 / 3352\rCalculating perplexity: 1601 / 3352\rCalculating perplexity: 1701 / 3352\rCalculating perplexity: 1801 / 3352\rCalculating perplexity: 1901 / 3352\rCalculating perplexity: 2001 / 3352\rCalculating perplexity: 2101 / 3352\rCalculating perplexity: 2201 / 3352\rCalculating perplexity: 2301 / 3352\rCalculating perplexity: 2401 / 3352\rCalculating perplexity: 2501 / 3352\rCalculating perplexity: 2601 / 3352\rCalculating perplexity: 2701 / 3352\rCalculating perplexity: 2801 / 3352\rCalculating perplexity: 2901 / 3352\rCalculating perplexity: 3001 / 3352\rCalculating perplexity: 3101 / 3352\rCalculating perplexity: 3201 / 3352\rCalculating perplexity: 3301 / 3352\rCalculating perplexity: 3352 / 3352\n",
            "\n",
            "The perplexity of Bigram is: 325.8354\n"
          ]
        }
      ],
      "source": [
        "bigram = BiGram(unigram_count, bigram_count)\n",
        "\n",
        "# Perplexity\n",
        "bigram_perplexity = perplexity(bigram, valid_token_ids)\n",
        "print(f'The perplexity of Bigram is: {bigram_perplexity:.4f}')"
      ],
      "id": "A2GFrhGgScZB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaOdd_cOScZB"
      },
      "source": [
        "If you implement correctly, the perplexity of good turing will be around 130"
      ],
      "id": "YaOdd_cOScZB"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcIB1LKqScZB",
        "outputId": "b1d4a1e3-964e-4382-aa79-1c801ffea87f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating perplexity: 3352 / 3352\n",
            "\n",
            "The perplexity of Good Turing is: 128.6806\n"
          ]
        }
      ],
      "source": [
        "gt = GoodTuring(unigram_count, bigram_count, threshold=100)\n",
        "\n",
        "# Perplexity\n",
        "gt_perplexity = perplexity(gt, valid_token_ids)\n",
        "print(f'The perplexity of Good Turing is: {gt_perplexity:.4f}')"
      ],
      "id": "vcIB1LKqScZB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JErN1PDrScZB"
      },
      "source": [
        "If you implement correctly, the perplexity of good turing will be around 60"
      ],
      "id": "JErN1PDrScZB"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL3469_fScZB",
        "outputId": "cc6b88d1-a8b5-4e49-855c-3f2b0a466ef4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rCalculating perplexity: 1 / 3352\rCalculating perplexity: 101 / 3352\rCalculating perplexity: 201 / 3352\rCalculating perplexity: 301 / 3352\rCalculating perplexity: 401 / 3352\rCalculating perplexity: 501 / 3352\rCalculating perplexity: 601 / 3352\rCalculating perplexity: 701 / 3352\rCalculating perplexity: 801 / 3352\rCalculating perplexity: 901 / 3352\rCalculating perplexity: 1001 / 3352\rCalculating perplexity: 1101 / 3352\rCalculating perplexity: 1201 / 3352\rCalculating perplexity: 1301 / 3352\rCalculating perplexity: 1401 / 3352\rCalculating perplexity: 1501 / 3352\rCalculating perplexity: 1601 / 3352\rCalculating perplexity: 1701 / 3352\rCalculating perplexity: 1801 / 3352\rCalculating perplexity: 1901 / 3352\rCalculating perplexity: 2001 / 3352\rCalculating perplexity: 2101 / 3352\rCalculating perplexity: 2201 / 3352\rCalculating perplexity: 2301 / 3352\rCalculating perplexity: 2401 / 3352\rCalculating perplexity: 2501 / 3352\rCalculating perplexity: 2601 / 3352\rCalculating perplexity: 2701 / 3352\rCalculating perplexity: 2801 / 3352\rCalculating perplexity: 2901 / 3352\rCalculating perplexity: 3001 / 3352\rCalculating perplexity: 3101 / 3352\rCalculating perplexity: 3201 / 3352\rCalculating perplexity: 3301 / 3352\rCalculating perplexity: 3352 / 3352\n",
            "\n",
            "The perplexity of Kneser-Ney is: 62.5943\n"
          ]
        }
      ],
      "source": [
        "kn = KneserNey(unigram_count, bigram_count, d=0.75)\n",
        "\n",
        "# Perplexity\n",
        "kn_perplexity = perplexity(kn, valid_token_ids)\n",
        "print(f'The perplexity of Kneser-Ney is: {kn_perplexity:.4f}')"
      ],
      "id": "EL3469_fScZB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4buYki4lScZB"
      },
      "source": [
        "### 1.7 Predict the next word given a previous word (5 points)"
      ],
      "id": "4buYki4lScZB"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "jWMM76tjScZB"
      },
      "outputs": [],
      "source": [
        "def predict(model: 'BiGram', w1: int, vocab_size: int):\n",
        "    \"\"\"Predict the w2 with the highest probability given w1.\n",
        "\n",
        "    Args:\n",
        "        model: A BiGram, GoodTuring, or KneserNey model that has the calc_prob function.\n",
        "        w1: Current word (token id).\n",
        "        vocab_size: The number of tokens in the vocabulary.\n",
        "\n",
        "    Returns:\n",
        "        result: The predicted token id for w2.\n",
        "    \"\"\"\n",
        "    result = None\n",
        "    highest_prob = 0\n",
        "    for w2 in range(1, vocab_size):\n",
        "        prob = model.calc_prob(w1, w2)\n",
        "        if prob > highest_prob:\n",
        "            highest_prob = prob\n",
        "            result = w2\n",
        "    return result\n"
      ],
      "id": "jWMM76tjScZB"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E0rocHuScZB",
        "outputId": "af13fe6a-01a9-4e7d-d671-7ca172614d19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sharply falling stock prices do reduce consumer wealth damage business ____\n",
            "predicted last token: </s>\n",
            "---------------------------------------------\n",
            "but robert an official of the association said no ____\n",
            "predicted last token: longer\n",
            "---------------------------------------------\n",
            "it also has interests in military electronics and marine ____\n",
            "predicted last token: s\n",
            "---------------------------------------------\n",
            "first chicago since n has reduced its loans to such ____\n",
            "predicted last token: as\n",
            "---------------------------------------------\n",
            "david m jones vice president at g ____\n",
            "predicted last token: s\n",
            "---------------------------------------------\n",
            "the n stock specialist firms on the big board floor ____\n",
            "predicted last token: traders\n",
            "---------------------------------------------\n",
            "at the same time the business was hurt by ____\n",
            "predicted last token: the\n",
            "---------------------------------------------\n",
            "salomon will cover the warrants by buying sufficient shares or ____\n",
            "predicted last token: n\n",
            "---------------------------------------------\n",
            "in july southmark corp the dallas based real estate and financial ____\n",
            "predicted last token: officer\n",
            "---------------------------------------------\n",
            "he concluded his remarks by and at some ____\n",
            "predicted last token: of\n",
            "---------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(12345)\n",
        "\n",
        "vocab_size = len(tokenizer)\n",
        "indexes = np.random.choice(len(test_token_ids), 10, replace=False)\n",
        "for i in indexes:\n",
        "    token_ids = test_token_ids[i][1:-1]\n",
        "    print(' '.join([tokenizer.inverse_vocab[token_id] for token_id in token_ids]) + ' ____')\n",
        "    pred = predict(gt, token_ids[-1], vocab_size)\n",
        "    print(f'predicted last token: {tokenizer.inverse_vocab[pred]}')\n",
        "    print('---------------------------------------------')"
      ],
      "id": "2E0rocHuScZB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIwVJB20ScZB"
      },
      "source": [
        "## Part B: 2. RNN (35 Points)"
      ],
      "id": "bIwVJB20ScZB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIzR0DEwScZB"
      },
      "source": [
        "### 2.1 Split feature and label"
      ],
      "id": "VIzR0DEwScZB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26nHWN-NScZB"
      },
      "outputs": [],
      "source": [
        "def get_feature_label(token_ids: List[List[int]], window_size: int=-1):\n",
        "    \"\"\" Split features and labels for the training, validation, and test datasets.\n",
        "\n",
        "    Note:\n",
        "        If window size is -1, for a sentence with n tokens,\n",
        "            it selects the tokens rangeing from [0, n - 1) as the feature,\n",
        "            and selects tokens ranging from [1, n) as the label.\n",
        "        Otherwise, it divides a sentence with multiple windows and do the previous split.\n",
        "    \"\"\"\n",
        "    x = []\n",
        "    y = []\n",
        "    seq_lens = []\n",
        "    for sent_token_ids in token_ids:\n",
        "        if window_size == -1:\n",
        "            x.append(sent_token_ids[:-1])\n",
        "            y.append(sent_token_ids[1:])\n",
        "            seq_lens.append(len(sent_token_ids) - 1)\n",
        "        else:\n",
        "            if len(sent_token_ids) > window_size:\n",
        "                sub_sent_size = window_size + 1\n",
        "                n_window = len(sent_token_ids) // (sub_sent_size)\n",
        "                for i in range(n_window):\n",
        "                    start = i * sub_sent_size\n",
        "                    sub_sent = sent_token_ids[start:(start + sub_sent_size)]\n",
        "                    x.append(sub_sent[:-1])\n",
        "                    y.append(sub_sent[1:])\n",
        "                    seq_lens.append(len(sub_sent) - 1)\n",
        "                if len(sent_token_ids) % sub_sent_size > 0:\n",
        "                    sub_sent = sent_token_ids[-sub_sent_size:]\n",
        "                    x.append(sub_sent[:-1])\n",
        "                    y.append(sub_sent[1:])\n",
        "                    seq_lens.append(len(sub_sent) - 1)\n",
        "            else:\n",
        "                x.append(sent_token_ids[:-1])\n",
        "                y.append(sent_token_ids[1:])\n",
        "                seq_lens.append(len(sent_token_ids) - 1)\n",
        "    return x, y, seq_lens"
      ],
      "id": "26nHWN-NScZB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1sDJGaBScZB",
        "outputId": "68b7ac06-cf7a-4204-90e2-1e0612daada8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40 68 68\n"
          ]
        }
      ],
      "source": [
        "window_size = 40\n",
        "x_train, y_train, train_seq_lens = get_feature_label(train_token_ids, window_size)\n",
        "x_valid, y_valid, valid_seq_lens = get_feature_label(valid_token_ids)\n",
        "x_test, y_test, test_seq_lens = get_feature_label(valid_token_ids)\n",
        "print(max(train_seq_lens), max(valid_seq_lens), max(test_seq_lens))"
      ],
      "id": "v1sDJGaBScZB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNgmMv9eScZN"
      },
      "source": [
        "### 2.2 Pad sentences in a batch to equal length (5 points)"
      ],
      "id": "VNgmMv9eScZN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVEpL8cfScZN"
      },
      "outputs": [],
      "source": [
        "def pad_batch(x_batch: List[List[int]], y_batch: List[List[int]], seq_lens_batch: List[int], pad_val: int):\n",
        "    \"\"\"Pad the sentences in a batch with pad_val based on the longest sentence.\n",
        "\n",
        "    Args:\n",
        "        x_batch, y_batch, seq_lens_batch: The input data (lists of token id lists and their lengths)\n",
        "        pad_val: The padding value to use.\n",
        "\n",
        "    Returns:\n",
        "        x_batch: Tensor of shape (batch_size, max_seq_len)\n",
        "        y_batch: Tensor of shape (batch_size, max_seq_len)\n",
        "        seq_lens_batch: Tensor of shape (batch_size,)\n",
        "    \"\"\"\n",
        "    max_len = max(seq_lens_batch)\n",
        "\n",
        "    padded_x_batch = [seq + [pad_val] * (max_len - len(seq)) for seq in x_batch]\n",
        "    padded_y_batch = [seq + [pad_val] * (max_len - len(seq)) for seq in y_batch]\n",
        "\n",
        "    x_tensor = tf.convert_to_tensor(padded_x_batch, dtype=tf.int64)\n",
        "    y_tensor = tf.convert_to_tensor(padded_y_batch, dtype=tf.int64)\n",
        "    seq_lens_tensor = tf.convert_to_tensor(seq_lens_batch, dtype=tf.int64)\n",
        "\n",
        "    return x_tensor, y_tensor, seq_lens_tensor\n"
      ],
      "id": "aVEpL8cfScZN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0VhFvSkScZN"
      },
      "source": [
        "### 2.3 RNN language model (10 points)"
      ],
      "id": "-0VhFvSkScZN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRWQkFs4ScZN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "\n",
        "class RNN(Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_units):\n",
        "        \"\"\"Init of the RNN model\n",
        "\n",
        "        Args:\n",
        "            vocab_size, embedding_dim: Used for initializing the embedding layer.\n",
        "            hidden_units: Number of hidden units of the RNN layer.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        #embedding layer.\n",
        "        self.embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
        "        # simple RNN layer that returns sequences.\n",
        "        self.rnn = SimpleRNN(units=hidden_units, return_sequences=True)\n",
        "        #dense layer that maps the RNN output to logits over the vocabulary.\n",
        "        self.dense = Dense(vocab_size)\n",
        "\n",
        "    def call(self, x):\n",
        "        \"\"\"Forward pass of the RNN model.\n",
        "\n",
        "        Args:\n",
        "            x: Tensor, (batch_size x max_seq_len). Input tokens.\n",
        "               Here, max_seq_len is the longest sentence length in this batch because we did pad_batch.\n",
        "        Returns:\n",
        "            outputs: Tensor, (batch_size x max_seq_len x vocab_size). Logits for every time step.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        x_emb = self.embedding(x)\n",
        "\n",
        "        rnn_ot = self.rnn(x_emb)\n",
        "\n",
        "        outputs = self.dense(rnn_ot)\n",
        "        return outputs\n"
      ],
      "id": "uRWQkFs4ScZN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKIVtXNgScZN"
      },
      "source": [
        "### 2.4 Seq2seq loss (5 Points)"
      ],
      "id": "qKIVtXNgScZN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIPjGTqFScZN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def seq2seq_loss(logits, target, seq_lens):\n",
        "    \"\"\"\n",
        "    Calculate the sequence-to-sequence loss using sparse categorical crossentropy,\n",
        "    with a mask to ignore padded timesteps.\n",
        "\n",
        "    Args:\n",
        "        logits: Tensor of shape (batch_size, max_seq_len, vocab_size). The output logits from the model.\n",
        "        target: Tensor of shape (batch_size, max_seq_len). The ground-truth token ids.\n",
        "        seq_lens: Tensor of shape (batch_size,). The actual sequence lengths (before padding).\n",
        "\n",
        "    Returns:\n",
        "        loss: A scalar tensor representing the average sequence loss.\n",
        "    \"\"\"\n",
        "    # Create a mask with 1's for valid timesteps and 0's for padded positions.\n",
        "    ma = tf.sequence_mask(seq_lens, maxlen=tf.shape(target)[1], dtype=tf.float32)\n",
        "\n",
        "\n",
        "    loss = tf.keras.losses.sparse_categorical_crossentropy(target, logits, from_logits=True)\n",
        "\n",
        "    # Apply the mask to zero-out losses from padded positions.\n",
        "    loss = loss * ma\n",
        "\n",
        "\n",
        "    loss = tf.reduce_sum(loss) / tf.reduce_sum(ma)\n",
        "\n",
        "    return loss\n"
      ],
      "id": "eIPjGTqFScZN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXB4ZDZpScZN"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(tokenizer)\n",
        "hidden_units = 128\n",
        "embedding_dim = 64\n",
        "num_epoch = 30\n",
        "batch_size = 256"
      ],
      "id": "KXB4ZDZpScZN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRUuAruRScZN"
      },
      "outputs": [],
      "source": [
        "model = RNN(vocab_size, embedding_dim, hidden_units)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)"
      ],
      "id": "QRUuAruRScZN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMQ276-6ScZN"
      },
      "source": [
        "### 2.5 Train RNN"
      ],
      "id": "ZMQ276-6ScZN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x6-8lvQScZN"
      },
      "source": [
        "If you implement everything correctly, the finall loss will be around 5.2"
      ],
      "id": "9x6-8lvQScZN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCA2KzxJScZN",
        "outputId": "63d2e4df-ce2d-4552-c379-f5a6e41b8acf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 30 - Step 170 / 170 - train loss: 6.9330 - valid loss: 6.6278\n",
            "Epoch 2 / 30 - Step 170 / 170 - train loss: 6.6486 - valid loss: 6.5479\n",
            "Epoch 3 / 30 - Step 170 / 170 - train loss: 6.4931 - valid loss: 6.3515\n",
            "Epoch 4 / 30 - Step 170 / 170 - train loss: 6.2797 - valid loss: 6.1636\n",
            "Epoch 5 / 30 - Step 170 / 170 - train loss: 6.0923 - valid loss: 5.9945\n",
            "Epoch 6 / 30 - Step 170 / 170 - train loss: 5.9112 - valid loss: 5.8373\n",
            "Epoch 7 / 30 - Step 170 / 170 - train loss: 5.7523 - valid loss: 5.7204\n",
            "Epoch 8 / 30 - Step 170 / 170 - train loss: 5.6308 - valid loss: 5.6386\n",
            "Epoch 9 / 30 - Step 170 / 170 - train loss: 5.5310 - valid loss: 5.5654\n",
            "Epoch 10 / 30 - Step 170 / 170 - train loss: 5.4435 - valid loss: 5.5079\n",
            "Epoch 11 / 30 - Step 170 / 170 - train loss: 5.3656 - valid loss: 5.4591\n",
            "Epoch 12 / 30 - Step 170 / 170 - train loss: 5.2952 - valid loss: 5.4174\n",
            "Epoch 13 / 30 - Step 170 / 170 - train loss: 5.2323 - valid loss: 5.3821\n",
            "Epoch 14 / 30 - Step 170 / 170 - train loss: 5.1752 - valid loss: 5.3544\n",
            "Epoch 15 / 30 - Step 170 / 170 - train loss: 5.1220 - valid loss: 5.3281\n",
            "Epoch 16 / 30 - Step 170 / 170 - train loss: 5.0729 - valid loss: 5.3076\n",
            "Epoch 17 / 30 - Step 170 / 170 - train loss: 5.0280 - valid loss: 5.2920\n",
            "Epoch 18 / 30 - Step 170 / 170 - train loss: 4.9860 - valid loss: 5.2837\n",
            "Epoch 19 / 30 - Step 170 / 170 - train loss: 4.9476 - valid loss: 5.2740\n",
            "Epoch 20 / 30 - Step 170 / 170 - train loss: 4.9123 - valid loss: 5.2547\n",
            "Epoch 21 / 30 - Step 170 / 170 - train loss: 4.8791 - valid loss: 5.2446\n",
            "Epoch 22 / 30 - Step 170 / 170 - train loss: 4.8485 - valid loss: 5.2414\n",
            "Epoch 23 / 30 - Step 170 / 170 - train loss: 4.8174 - valid loss: 5.2348\n",
            "Epoch 24 / 30 - Step 170 / 170 - train loss: 4.7869 - valid loss: 5.2302\n",
            "Epoch 25 / 30 - Step 170 / 170 - train loss: 4.7557 - valid loss: 5.2277\n",
            "Epoch 26 / 30 - Step 170 / 170 - train loss: 4.7270 - valid loss: 5.2238\n",
            "Epoch 27 / 30 - Step 170 / 170 - train loss: 4.6999 - valid loss: 5.2228\n",
            "Epoch 28 / 30 - Step 170 / 170 - train loss: 4.6750 - valid loss: 5.2215\n",
            "Epoch 29 / 30 - Step 170 / 170 - train loss: 4.6500 - valid loss: 5.2223\n",
            "Epoch 30 / 30 - Step 170 / 170 - train loss: 4.6261 - valid loss: 5.2223\n"
          ]
        }
      ],
      "source": [
        "num_samples = len(x_train)\n",
        "n_batch = int(np.ceil(num_samples / batch_size))\n",
        "n_valid_batch = int(np.ceil(len(x_valid) / batch_size))\n",
        "for epoch in range(num_epoch):\n",
        "    epoch_loss = 0.0\n",
        "    for batch_idx in range(n_batch):\n",
        "        start = batch_idx * batch_size\n",
        "        end = start + batch_size\n",
        "        x_batch, y_batch, seq_lens_batch = x_train[start:end], y_train[start:end], train_seq_lens[start:end]\n",
        "        real_batch_size = len(x_batch)\n",
        "        x_batch, y_batch, seq_lens_batch = pad_batch(x_batch, y_batch, seq_lens_batch, pad_val=tokenizer.pad_token_id)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            output = model(x_batch)\n",
        "            loss = seq2seq_loss(output, y_batch, seq_lens_batch)\n",
        "\n",
        "        if batch_idx % 1 == 0 or batch_idx == num_samples - 1:\n",
        "            print_line(f'Epoch {epoch + 1} / {num_epoch} - Step {batch_idx + 1} / {n_batch} - loss: {loss:.4f}')\n",
        "\n",
        "        trainable_vars = model.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        epoch_loss += loss * real_batch_size\n",
        "\n",
        "    valid_loss = 0.0\n",
        "    for batch_idx in range(n_valid_batch):\n",
        "        start = batch_idx * batch_size\n",
        "        end = start + batch_size\n",
        "        x_batch, y_batch, seq_lens_batch = x_valid[start:end], y_valid[start:end], valid_seq_lens[start:end]\n",
        "        real_batch_size = len(x_batch)\n",
        "        x_batch, y_batch, seq_lens_batch = pad_batch(x_batch, y_batch, seq_lens_batch, pad_val=tokenizer.pad_token_id)\n",
        "        output = model(x_batch)\n",
        "        loss = seq2seq_loss(output, y_batch, seq_lens_batch)\n",
        "\n",
        "        if batch_idx % 1 == 0 or batch_idx == len(x_valid) - 1:\n",
        "            print_line(f'Epoch {epoch + 1} / {num_epoch} - Step {batch_idx + 1} / {n_valid_batch} - loss: {loss:.4f}')\n",
        "\n",
        "        valid_loss += loss * real_batch_size\n",
        "    print(f'\\rEpoch {epoch + 1} / {num_epoch} - Step {n_batch} / {n_batch} - train loss: {epoch_loss / num_samples:.4f} - valid loss: {valid_loss / len(x_valid):.4f}')"
      ],
      "id": "oCA2KzxJScZN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyqnutC8ScZN"
      },
      "source": [
        "### 2.6 Perplexity of RNN (10 points)"
      ],
      "id": "yyqnutC8ScZN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAT3WWOXScZN"
      },
      "source": [
        "Here,\n",
        "1. you need to calculate the perplexity based on its definition.\n",
        "2. Besides, you need to record the loss for every word prediction and calculate the sum of loss\n",
        "3. Finaly, you will need to compare the perplexity by definition and the perplexity by the loss: `np.exp(total_loss / n_words)`"
      ],
      "id": "pAT3WWOXScZN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xluPKS0gScZN",
        "outputId": "70c56ea5-7e0e-428e-b0c8-5bd63ff2b887"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating perplexity: 3352 / 3352\n",
            "\n",
            "Perplexity by definition: 185.2645, Perplexity by loss: 185.2645\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "n = len(x_valid)\n",
        "log_probs = 0\n",
        "n_words = 0      # Total number of word predictions.\n",
        "total_loss = 0   # Sum of cross-entropy losses\n",
        "\n",
        "for i in range(n):\n",
        "    if i % 1 == 0 or i == n - 1:\n",
        "        print_line('Calculating perplexity:', (i + 1), '/', n)\n",
        "\n",
        "    x_line, y_line, line_seq_lens = x_valid[i:i + 1], y_valid[i:i + 1], valid_seq_lens[i:i + 1]\n",
        "    x_line, y_line, line_seq_lens = pad_batch(x_line, y_line, line_seq_lens, tokenizer.pad_token_id)\n",
        "\n",
        "\n",
        "    output = model(x_line)\n",
        "    pred_probs = tf.nn.softmax(output, axis=-1)\n",
        "\n",
        "\n",
        "    # We assume y_line[0] is a list/tensor of token ids for the sentence.\n",
        "    for real_token, probs in zip(y_line[0].numpy(), pred_probs[0].numpy()):\n",
        "\n",
        "        # Convert the real token id to an int and get the probability assigned\n",
        "        p = probs[int(real_token)]\n",
        "\n",
        "\n",
        "        log_probs += math.log(p)\n",
        "\n",
        "        total_loss += -math.log(p)\n",
        "        n_words += 1\n",
        "\n",
        "perplexity_by_def = np.exp(-log_probs / n_words)\n",
        "perplexity_by_loss = np.exp(total_loss / n_words)\n",
        "\n",
        "print('\\n')\n",
        "print(f'Perplexity by definition: {perplexity_by_def:.4f}, Perplexity by loss: {perplexity_by_loss:.4f}')\n"
      ],
      "id": "xluPKS0gScZN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q-Lj1hZScZO"
      },
      "source": [
        "### 2.7 Predict the next word given a previous sentence (5 Points)"
      ],
      "id": "7Q-Lj1hZScZO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PK4hFpH1ScZO",
        "outputId": "282fb96e-6296-4085-f5e1-664b329cc7ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sharply falling stock prices do reduce consumer wealth damage business ____\n",
            "predicted last token: </s>\n",
            "---------------------------------------------\n",
            "but robert an official of the association said no ____\n",
            "predicted last token: one\n",
            "---------------------------------------------\n",
            "it also has interests in military electronics and marine ____\n",
            "predicted last token: and\n",
            "---------------------------------------------\n",
            "first chicago since n has reduced its loans to such ____\n",
            "predicted last token: as\n",
            "---------------------------------------------\n",
            "david m jones vice president at g ____\n",
            "predicted last token: s\n",
            "---------------------------------------------\n",
            "the n stock specialist firms on the big board floor ____\n",
            "predicted last token: were\n",
            "---------------------------------------------\n",
            "at the same time the business was hurt by ____\n",
            "predicted last token: the\n",
            "---------------------------------------------\n",
            "salomon will cover the warrants by buying sufficient shares or ____\n",
            "predicted last token: n\n",
            "---------------------------------------------\n",
            "in july southmark corp the dallas based real estate and financial ____\n",
            "predicted last token: services\n",
            "---------------------------------------------\n",
            "he concluded his remarks by and at some ____\n",
            "predicted last token: of\n",
            "---------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def predict_next_token(model, token_ids):\n",
        "\n",
        "    # Convert token_ids to a tensor with a batch dimension.\n",
        "    x = tf.convert_to_tensor([token_ids], dtype=tf.int64)\n",
        "\n",
        "    logits = model(x)\n",
        "    # Take the logits from the last time step.\n",
        "    last_logits = logits[0, -1, :]\n",
        "\n",
        "\n",
        "    pred_token = tf.argmax(last_logits).numpy()\n",
        "    return pred_token\n",
        "\n",
        "np.random.seed(12345)\n",
        "vocab_size = len(tokenizer)\n",
        "indexes = np.random.choice(len(test_token_ids), 10, replace=False)\n",
        "for i in indexes:\n",
        "    token_ids = test_token_ids[i][1:-1]\n",
        "    print(' '.join([tokenizer.inverse_vocab[token_id] for token_id in token_ids]) + ' ____')\n",
        "    pred = predict_next_token(model, token_ids)\n",
        "    print(f'predicted last token: {tokenizer.inverse_vocab[pred]}')\n",
        "    print('---------------------------------------------')\n"
      ],
      "id": "PK4hFpH1ScZO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj8OxDnvBrNK"
      },
      "source": [
        "## 3. Conclusion (5 points)"
      ],
      "id": "Yj8OxDnvBrNK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SnZhw_oScZO"
      },
      "source": [
        "Briefly analyze the result of N-Gram and RNN\n",
        "\n",
        "\n",
        "N-Gram Model:\n",
        "\n",
        "\n",
        "*   Strengths: Simple, fast, good for common word pairs and short-range dependencies.\n",
        "\n",
        "* Weaknesses: Struggles with longer-range dependencies, has high perplexity, and lacks flexibility in handling complex or rare sequences.\n",
        "\n",
        "\n",
        "Perplexity: 325.8354, indicating uncertainty in predictions.\n",
        "\n",
        "RNN Model:\n",
        "\n",
        "\n",
        "\n",
        "* Strengths: Captures long-range dependencies, adapts over time with training,and performs better in complex sequence generation.\n",
        "* Weaknesses: Computationally expensive, slower training, and still struggles with perplexity.  \n",
        "\n",
        "\n",
        "Perplexity: 185.2645, lower than the N-Gram model, indicating better prediction consistency.\n",
        "\n",
        "\n",
        "**Conclusion: **\n",
        "The RNN model outperforms the N-Gram model in handling long-range dependencies and prediction accuracy, though both models still have room for improvement due to their high perplexity. The N-Gram model is faster but kinda limited in its ability to handle complex contexts."
      ],
      "id": "5SnZhw_oScZO"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}